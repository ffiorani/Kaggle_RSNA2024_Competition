{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":71549,"databundleVersionId":8561470,"sourceType":"competition"},{"sourceId":198273584,"sourceType":"kernelVersion"}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom glob import glob\nfrom PIL import Image\nfrom tqdm import tqdm\nimport math\nimport seaborn as sns\nimport random\nfrom collections import OrderedDict\nfrom sklearn.model_selection import KFold\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import AdamW\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport timm\nfrom transformers import get_cosine_schedule_with_warmup\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport matplotlib.patches as patches\nimport torchvision.utils as vutils","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-10-02T09:19:34.696642Z","iopub.execute_input":"2024-10-02T09:19:34.696923Z","iopub.status.idle":"2024-10-02T09:19:44.117952Z","shell.execute_reply.started":"2024-10-02T09:19:34.696886Z","shell.execute_reply":"2024-10-02T09:19:44.116840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BASE_URL = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/'\nOUTPUT_DIR = 'rsna24-results'\nIMAGE_URL = '/kaggle/input/crop-and-convert-to-png-axial/Converted_images/'\nLEVELS = ['L1/L2', 'L2/L3', 'L3/L4', 'L4/L5', 'L5/S1']\nSEED = 8620\nDEBUG = False # if set to true, run fewer computations\nif DEBUG:\n    import unittest\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nN_WORKERS = os.cpu_count()\n\n\nN_LABELS = 25\nN_CLASSES = 3 * N_LABELS\n\nN_FOLDS = 4 if not DEBUG else 2\nEPOCHS = 30 if not DEBUG else 20\nMODEL_NAME = 'resnet18' if DEBUG else 'densenet161.tv_in1k'\n\nGRAD_ACCUMULATION = 4\nTARGET_BATCH_SIZE = 16\nBATCH_SIZE = TARGET_BATCH_SIZE // GRAD_ACCUMULATION\nMAX_GRAD_NORM = 1\nEARLY_STOPPING_EPOCH = 6\n\nLEARNING_RATE = 2e-4 * TARGET_BATCH_SIZE / 32\nWEIGHT_DECAY = 1e-2\nAUGMENTATION = True\n\nUSE_AUTOMATIC_MIXED_PRECISION = True # can change True if using T4 or newer than Ampere\nAUGMENTATION_PROBABILITY = 0.75\n\nSAGITTAL_IMAGE_SHAPE = (490, 275) # based on an image that was (620, 620), height first then width\nAXIAL_IMAGE_SHAPE = (310, 250) # based on an image that was (620, 620), height first then width","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:25:15.460705Z","iopub.execute_input":"2024-10-02T09:25:15.461458Z","iopub.status.idle":"2024-10-02T09:25:15.470371Z","shell.execute_reply.started":"2024-10-02T09:25:15.461405Z","shell.execute_reply":"2024-10-02T09:25:15.469340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.makedirs(OUTPUT_DIR, exist_ok = True)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:19:44.160093Z","iopub.execute_input":"2024-10-02T09:19:44.160931Z","iopub.status.idle":"2024-10-02T09:19:44.172247Z","shell.execute_reply.started":"2024-10-02T09:19:44.160897Z","shell.execute_reply":"2024-10-02T09:19:44.171296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_random_seed(seed: int = 8620, deterministic: bool = False):\n    \"\"\"Set seeds\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)  # type: ignore\n    torch.backends.cudnn.benchmark = True\n    torch.backends.cudnn.deterministic = deterministic  # type: ignore\n\nset_random_seed(SEED)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:19:44.175251Z","iopub.execute_input":"2024-10-02T09:19:44.175634Z","iopub.status.idle":"2024-10-02T09:19:44.189741Z","shell.execute_reply.started":"2024-10-02T09:19:44.175593Z","shell.execute_reply":"2024-10-02T09:19:44.188891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(f'{BASE_URL}train.csv')\ntrain_descriptions = pd.read_csv(f'{BASE_URL}train_series_descriptions.csv')\ncoordinates_df = pd.read_csv(f'{BASE_URL}train_label_coordinates.csv')\ncoordinates_df = pd.merge(coordinates_df, train_descriptions, on = ['study_id', 'series_id'])\ncoordinates_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:19:44.191153Z","iopub.execute_input":"2024-10-02T09:19:44.191491Z","iopub.status.idle":"2024-10-02T09:19:44.451360Z","shell.execute_reply.started":"2024-10-02T09:19:44.191460Z","shell.execute_reply":"2024-10-02T09:19:44.450322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sagittal_t2_coordinates = pd.read_csv('/kaggle/input/crop-and-convert-to-png-axial/Updated_coordinates/new_sagittal_t2_coordinates.csv')\nsagittal_t1_coordinates = pd.read_csv('/kaggle/input/crop-and-convert-to-png-axial/Updated_coordinates/new_sagittal_t1_coordinates.csv')\naxial_t2_coordinates = pd.read_csv('/kaggle/input/crop-and-convert-to-png-axial/Updated_coordinates/new_axial_t2_coordinates.csv')","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:19:44.453683Z","iopub.execute_input":"2024-10-02T09:19:44.454727Z","iopub.status.idle":"2024-10-02T09:19:44.681812Z","shell.execute_reply.started":"2024-10-02T09:19:44.454682Z","shell.execute_reply":"2024-10-02T09:19:44.680877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sorting the coordinates dataframes","metadata":{}},{"cell_type":"code","source":"sagittal_t2_coordinates.sort_values(by = ['study_id', 'level', 'condition'], inplace = True)\nsagittal_t1_coordinates.sort_values(by = ['study_id', 'condition', 'level'], ascending = [True, False, True], inplace = True)\naxial_t2_coordinates.sort_values(by = ['study_id', 'level', 'condition'], ascending = [True, True, False], inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:19:44.683181Z","iopub.execute_input":"2024-10-02T09:19:44.684050Z","iopub.status.idle":"2024-10-02T09:19:44.714665Z","shell.execute_reply.started":"2024-10-02T09:19:44.684003Z","shell.execute_reply":"2024-10-02T09:19:44.713830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"condition_encoding = {'Normal/Mild': 0, 'Moderate': 1, 'Severe': 2}\n\ndef map_condition(condition):\n    return condition_encoding.get(condition, -100) # we need to make sure that these NA filled with -100 are not used\n\ntrain_df.iloc[:, 1:] = train_df.iloc[:, 1:].map(map_condition)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:19:44.715801Z","iopub.execute_input":"2024-10-02T09:19:44.716094Z","iopub.status.idle":"2024-10-02T09:19:44.779189Z","shell.execute_reply.started":"2024-10-02T09:19:44.716063Z","shell.execute_reply":"2024-10-02T09:19:44.778230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Getting study ids that have all the coordinates","metadata":{}},{"cell_type":"code","source":"def get_side(instance_number, num_images):\n    return 'Right' if instance_number < num_images // 2 else 'Left'\n\ndef get_num_images(study_id, series_id):\n    image_paths = glob(f'{BASE_URL}train_images/{study_id}/{series_id}/*dcm')\n    return len(image_paths)\n\ndef filter_study_ids(sagittal_t2_coordinates, sagittal_t1_coordinates, axial_t2_coordinates):\n    valid_study_ids = set(sagittal_t2_coordinates['study_id'].unique())\n\n    valid_sagittal_t2_studies = sagittal_t2_coordinates.groupby('study_id').filter(\n        lambda x: set(x['level']) == set(['L1/L2', 'L2/L3', 'L3/L4', 'L4/L5', 'L5/S1'])\n    )['study_id'].unique()\n\n    valid_sagittal_t1_studies = sagittal_t1_coordinates.groupby('study_id').filter(\n        lambda x: set(x['level']) == set(['L1/L2', 'L2/L3', 'L3/L4', 'L4/L5', 'L5/S1'])\n    )['study_id'].unique()\n\n    valid_axial_t2_studies = axial_t2_coordinates.groupby('study_id').filter(\n        lambda x: set(x['level']) == set(['L1/L2', 'L2/L3', 'L3/L4', 'L4/L5', 'L5/S1'])\n    )['study_id'].unique()\n\n    valid_study_ids = set(valid_sagittal_t2_studies) & set(valid_sagittal_t1_studies) & set(valid_axial_t2_studies)\n    \n    return valid_study_ids\n\ndef filter_sides(sagittal_t1_coordinates, axial_t2_coordinates, valid_study_ids):\n    final_study_ids = set()\n    \n    for study_id in valid_study_ids:\n        sagittal_t1_study = sagittal_t1_coordinates[sagittal_t1_coordinates['study_id'] == study_id]\n        axial_t2_study = axial_t2_coordinates[axial_t2_coordinates['study_id'] == study_id]\n        \n        sagittal_t1_sides = set()\n        axial_t2_sides = set()\n        \n        for level in LEVELS:\n            sagittal_t1_level = sagittal_t1_study[sagittal_t1_study['level'] == level]\n            axial_t2_level = axial_t2_study[axial_t2_study['level'] == level]\n            \n            if sagittal_t1_level.empty or axial_t2_level.empty:\n                continue\n            \n            num_images_sagittal_t1 = get_num_images(study_id, sagittal_t1_level.iloc[0]['series_id'])\n            sagittal_t1_level = sagittal_t1_level[sagittal_t1_level['instance_number'] < num_images_sagittal_t1]\n            sagittal_t1_sides.update(sagittal_t1_level.apply(lambda row: get_side(row['instance_number'], num_images_sagittal_t1), axis=1))\n            \n            num_images_axial_t2 = get_num_images(study_id, axial_t2_level.iloc[0]['series_id'])\n            axial_t2_level = axial_t2_level[axial_t2_level['instance_number'] < num_images_axial_t2]\n            axial_t2_sides.update(axial_t2_level['condition'].apply(lambda cond: cond.split()[0]))\n        \n        if sagittal_t1_sides == {'Right', 'Left'} and axial_t2_sides == {'Right', 'Left'}:\n            final_study_ids.add(study_id)\n    \n    return final_study_ids\n\nvalid_study_ids = filter_study_ids(sagittal_t2_coordinates, sagittal_t1_coordinates, axial_t2_coordinates)\nfinal_study_ids = filter_sides(sagittal_t1_coordinates, axial_t2_coordinates, valid_study_ids)\n\nall_labels_df = train_df[train_df['study_id'].isin(final_study_ids)]\nall_labels_df\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:19:44.780700Z","iopub.execute_input":"2024-10-02T09:19:44.781321Z","iopub.status.idle":"2024-10-02T09:20:42.943224Z","shell.execute_reply.started":"2024-10-02T09:19:44.781261Z","shell.execute_reply":"2024-10-02T09:20:42.942255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataloader","metadata":{}},{"cell_type":"code","source":"class MultiTaskDataset(Dataset):\n    def __init__(\n        self, \n        label_df = train_df,\n        series_descriptions = train_descriptions,\n        sagittal_t2_coordinates = sagittal_t2_coordinates, \n        sagittal_t1_coordinates = sagittal_t1_coordinates, \n        axial_t2_coordinates = axial_t2_coordinates, \n        phase = 'train', \n        transform = None\n    ):\n        self.label_df = label_df\n        self.series_descriptions = series_descriptions\n        self.sagittal_t2_coordinates = sagittal_t2_coordinates\n        self.sagittal_t1_coordinates = sagittal_t1_coordinates\n        self.axial_t2_coordinates = axial_t2_coordinates\n        self.transform = transform\n        self.phase = phase\n        self.levels = ['L1/L2', 'L2/L3', 'L3/L4', 'L4/L5', 'L5/S1']\n        self.sides = ['right', 'left']\n\n        # Initialize dictionaries\n        self.images = {}\n        self.keypoints = {}\n        self.masks = {}\n        \n        self.PILToTensor = transforms.Compose([transforms.PILToTensor()])\n        # Preload images\n        self.preload_images()\n\n    def preload_images(self):\n        study_ids = self.label_df['study_id']\n        for idx in range(len(self.label_df)):\n            study_id = study_ids.iloc[idx]\n            if study_id not in self.images:\n                self.images[study_id] = {}\n                self.keypoints[study_id] = {}\n                self.masks[study_id] = {}\n                \n                # loading Sagittal T2\n                self._load_sagittal_t2(study_id)\n                \n                # loading Sagittal T1\n                self._load_sagittal_t1(study_id)\n                \n                # loading Axial T2\n                self._load_axial_t2(study_id)\n\n    def _load_sagittal_t2(self, study_id):\n        description = 'Sagittal T2/STIR'\n        series_id_df = self.series_descriptions.query('@study_id == study_id and @description == series_description')\n        study_id_coordinates = self.sagittal_t2_coordinates.query('@study_id == study_id')\n        self.keypoints[study_id]['Sagittal T2'] = {level: [] for level in self.levels}\n        self.masks[study_id]['Sagittal T2'] = 0\n\n        if not series_id_df.empty:\n            series_id = series_id_df['series_id'].iloc[0]\n            image_paths = glob(f'{IMAGE_URL}{study_id}/Sagittal_T2/*.png')\n            try:\n                self.images[study_id]['Sagittal T2'] = torch.stack([self.PILToTensor(Image.open(path).convert('RGB')) for path in image_paths], dim=1).squeeze()\n            except Exception as e:\n                print(f'Study id: {study_id} Sagittal T2 error while loading image: {str(e)}')\n            coordinates_dict = {}\n            for level in self.levels:\n                level_coords = study_id_coordinates.query('level == @level')\n                if level_coords.empty:\n                    coordinates_dict[level] = []\n                else:\n                    coordinates_dict[level] = level_coords[['x', 'y']].astype(float).values.tolist()\n            \n            self.keypoints[study_id]['Sagittal T2'] = coordinates_dict\n            self.masks[study_id]['Sagittal T2'] = 1\n        else:\n            self.images[study_id]['Sagittal T2'] = torch.zeros((3, SAGITTAL_IMAGE_SHAPE[0], SAGITTAL_IMAGE_SHAPE[1]))\n            self.keypoints[study_id]['Sagittal T2'] = {level: [] for level in self.levels}\n            self.masks[study_id]['Sagittal T2'] = 0\n\n    def _load_sagittal_t1(self, study_id):\n        description = 'Sagittal T1'\n        series_ids = self.series_descriptions.query('@study_id == study_id and @description == series_description')\n        self.images[study_id]['Sagittal T1'] = []\n        self.keypoints[study_id]['Sagittal T1'] = {side: {level: [] for level in self.levels} for side in self.sides}\n        self.masks[study_id]['Sagittal T1'] = {side: {level: 0 for level in self.levels} for side in self.sides}\n        for side in self.sides:\n            if not series_ids.empty:\n                series_id = series_ids['series_id'].iloc[0]\n                series_id_coordinates = self.sagittal_t1_coordinates.query('@study_id == study_id and @series_id == series_id and condition.str.startswith(@side.capitalize())')\n                image_paths = glob(f'{IMAGE_URL}{study_id}/Sagittal_T1/{side.lower()}.png')\n                if image_paths:\n                    self.images[study_id]['Sagittal T1'].append(torch.stack([self.PILToTensor(Image.open(path).convert('RGB')) for path in image_paths], dim=1).squeeze())\n                    side_keypoints = {level: series_id_coordinates.query('level == @level')[['x', 'y']].astype(float).values.tolist() for level in self.levels}\n                    self.keypoints[study_id]['Sagittal T1'][side] = side_keypoints\n                    self.masks[study_id]['Sagittal T1'][side] = 1\n                else:\n                    self.images[study_id]['Sagittal T1'].append(torch.zeros((3, SAGITTAL_IMAGE_SHAPE[0], SAGITTAL_IMAGE_SHAPE[1])))\n                    self.keypoints[study_id]['Sagittal T1'][side] = {level: [] for level in self.levels}\n                    self.masks[study_id]['Sagittal T1'][side] = 0\n            else:\n                self.images[study_id]['Sagittal T1'].append(torch.zeros((3, SAGITTAL_IMAGE_SHAPE[0], SAGITTAL_IMAGE_SHAPE[1])))\n                self.keypoints[study_id]['Sagittal T1'][side] = {level: [] for level in self.levels}\n                self.masks[study_id]['Sagittal T1'][side] = 0\n\n    def _load_axial_t2(self, study_id):\n        description = 'Axial T2'\n        series_id = self.series_descriptions.query('@study_id == study_id and @description == series_description')\n        self.images[study_id]['Axial T2'] = []\n        self.keypoints[study_id]['Axial T2'] = {level: {side: [] for side in self.sides} for level in self.levels}\n        self.masks[study_id]['Axial T2'] = {level: {side: 0 for side in self.sides} for level in self.levels}\n        for level in self.levels:\n            if not series_id.empty:\n                level_path = level.replace('/', '-')\n                image_paths = glob(f'{IMAGE_URL}{study_id}/Axial_T2/{level_path}.png')\n                if image_paths:\n                    self.images[study_id]['Axial T2'].append(torch.stack([self.PILToTensor(Image.open(path).convert('RGB')) for path in image_paths], dim=1).squeeze())\n                    level_keypoints = {}\n                    for side in self.sides:\n                        query_result = self.axial_t2_coordinates.query('@study_id == study_id and level == @level and condition.str.startswith(@side.capitalize())')\n                        if not query_result.empty:\n                            level_keypoints[side] = query_result[['x', 'y']].astype(float).values.tolist()\n                        else:\n                            level_keypoints[side] = []\n                            \n                    self.keypoints[study_id]['Axial T2'][level] = level_keypoints\n                    self.masks[study_id]['Axial T2'][level] = 1\n                else:\n                    self.images[study_id]['Axial T2'].append(torch.zeros((3, AXIAL_IMAGE_SHAPE[0], AXIAL_IMAGE_SHAPE[1])))\n                    self.keypoints[study_id]['Axial T2'][level] = {side: [] for side in self.sides}\n                    self.masks[study_id]['Axial T2'][level] = 0\n            else:\n                self.images[study_id]['Axial T2'].append(torch.zeros((3, AXIAL_IMAGE_SHAPE[0], AXIAL_IMAGE_SHAPE[1])))\n                self.keypoints[study_id]['Axial T2'][level] = {side: [] for side in self.sides}\n                self.masks[study_id]['Axial T2'][level] = 0\n\n    def extend_dataset(self, new_label_df):\n        # Extend the dataset with new samples\n        self.label_df = new_label_df\n        self.preload_images()  # This will only load new images\n\n    def __len__(self):\n        return len(self.label_df)\n    \n    def augment_image_and_keypoints(self, image_tensor, keypoints):\n        # Convert image tensor to numpy (H, W, C)\n        image_np = image_tensor.permute(1, 2, 0).cpu().numpy()\n        assert np.sum(image_np) > 0, \"Input image is all zeros\"\n        \n        # Flatten keypoints for augmentation\n        flat_keypoints = [point for level_points in keypoints.values() for point in level_points] # they should be side points for axial\n\n        # Apply augmentations to both image and keypoints\n        augmented = self.transform(image=image_np, keypoints=flat_keypoints)\n        augmented_image_tensor = augmented['image']\n\n        # Perform checks on the augmented image\n        assert augmented_image_tensor.numel() > 0, \"Augmented image is empty\"\n        assert torch.any(augmented_image_tensor != 0), \"Augmented image is all zeros\"\n        assert augmented_image_tensor.dtype == torch.float32, \"Augmented image is not float32\"\n        assert torch.all(augmented_image_tensor >= -3) and torch.all(augmented_image_tensor <= 3), \\\n            f\"Augmented image values out of expected range: min={augmented_image_tensor.min()}, max={augmented_image_tensor.max()}\"\n        assert augmented_image_tensor.shape[0] == 3 and augmented_image_tensor.shape[1] > 0 and augmented_image_tensor.shape[2] > 0, \\\n            f\"Unexpected augmented image shape: {augmented_image_tensor.shape}\"\n\n        # Reconstruct keypoints dictionary\n        augmented_keypoints = augmented['keypoints']\n        reconstructed_keypoints = {}\n        idx = 0\n        for level, points in keypoints.items():\n            level_points = augmented_keypoints[idx:idx+len(points)]\n            # Check if keypoints are still in the image\n            valid_points = [point for point in level_points if 0 <= point[0] < augmented_image_tensor.shape[2] and 0 <= point[1] < augmented_image_tensor.shape[1]]\n            reconstructed_keypoints[level] = valid_points if valid_points else []\n            idx += len(points)\n        \n        # Perform checks on the reconstructed keypoints\n        assert len(reconstructed_keypoints) == len(keypoints), \"Number of keypoint levels changed after augmentation\"\n\n        return augmented_image_tensor, reconstructed_keypoints\n\n    def __getitem__(self, idx):\n        labels_row = self.label_df.iloc[idx]\n        study_id = labels_row['study_id']\n        labels = labels_row.iloc[1:].values.astype(np.int64)\n\n        # Get images and keypoints\n        sagittal_t2_image = self.images[study_id]['Sagittal T2']\n        sagittal_t1_images = self.images[study_id]['Sagittal T1']\n        axial_t2_images = self.images[study_id]['Axial T2']\n\n        sagittal_t2_keypoints = self.keypoints[study_id]['Sagittal T2']\n        sagittal_t1_keypoints = self.keypoints[study_id]['Sagittal T1']\n        axial_t2_keypoints = self.keypoints[study_id]['Axial T2']\n\n        # Get masks\n        mask = [\n            self.masks[study_id]['Sagittal T2'],\n            *self.masks[study_id]['Sagittal T1'].values(),\n            *self.masks[study_id]['Axial T2'].values()\n        ]\n        mask = torch.tensor(mask, dtype=torch.float32)\n\n        # applying transformations\n        if self.transform is not None:\n            if self.masks[study_id]['Sagittal T2'] == 1:\n                sagittal_t2_image, sagittal_t2_keypoints = self.augment_image_and_keypoints(sagittal_t2_image, sagittal_t2_keypoints)\n            \n            for i, side in enumerate(self.sides):\n                if self.masks[study_id]['Sagittal T1'][side] == 1:  \n                    sagittal_t1_images[i], sagittal_t1_keypoints[side] = self.augment_image_and_keypoints(sagittal_t1_images[i], sagittal_t1_keypoints[side])\n                    \n            for i, level in enumerate(self.levels):\n                if self.masks[study_id]['Axial T2'][level] == 1:\n                    axial_t2_images[i], axial_t2_keypoints[level] = self.augment_image_and_keypoints(axial_t2_images[i], axial_t2_keypoints[level])\n                    \n        x = torch.nested.nested_tensor([\n            sagittal_t2_image.unsqueeze(0),\n            torch.stack(sagittal_t1_images),\n            torch.stack(axial_t2_images)\n        ])\n            \n        assert x.numel() > 0, \"Transformed image is empty\"\n        assert any(torch.any(tensor != 0) for tensor in x.unbind()), \"Transformed image is all zeros\"\n        assert x.dtype == torch.float32, \"Transformed image is not float32\"\n        assert len([_ for _ in x.unbind()]) == 3  # Nested tensor has 3 elements: sagittal T2, sagittal T1, axial T2\n        assert x[0].shape == (1, 3, SAGITTAL_IMAGE_SHAPE[0], SAGITTAL_IMAGE_SHAPE[1])  # Sagittal T2\n        assert x[1].shape == (2, 3, SAGITTAL_IMAGE_SHAPE[0], SAGITTAL_IMAGE_SHAPE[1])  # Sagittal T1 (2 images)\n        assert x[2].shape == (5, 3, AXIAL_IMAGE_SHAPE[0], AXIAL_IMAGE_SHAPE[1])  # Axial T2 (5 images)\n        \n        \n        keypoints = {\n            'Sagittal T2': sagittal_t2_keypoints,\n            'Sagittal T1': sagittal_t1_keypoints,\n            'Axial T2': axial_t2_keypoints\n        }\n                    \n        return {\n            'images': x,\n            'labels': labels,\n            'keypoints': keypoints,\n            'mask': mask,\n            'study_id': study_id\n        }\n    \ndef custom_collate(batch):\n    # Separate the different items\n    images = [item['images'] for item in batch]\n    labels = [item['labels'] for item in batch]\n    keypoints = [item['keypoints'] for item in batch]\n    masks = [item['mask'] for item in batch]\n    study_ids = [item['study_id'] for item in batch]\n\n    # Create nested tensor for images, preserving the structure\n    nested_images = []\n    for i in range(len([_ for _ in images[0].unbind()])):  # For each image type\n        nested_images.append(torch.stack([sample[i] for sample in images]))\n    \n    images = torch.nested.nested_tensor(nested_images)\n\n    # Stack labels and masks\n    labels = torch.stack([torch.tensor(l) for l in labels])\n    masks = torch.stack(masks)\n\n    # Keypoints and study_ids can remain as lists\n\n    return {\n        'images': images,\n        'labels': labels,\n        'keypoints': keypoints,\n        'mask': masks,\n        'study_id': study_ids\n    }","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:20:42.946683Z","iopub.execute_input":"2024-10-02T09:20:42.947039Z","iopub.status.idle":"2024-10-02T09:20:43.001840Z","shell.execute_reply.started":"2024-10-02T09:20:42.947006Z","shell.execute_reply":"2024-10-02T09:20:43.000916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Augmentation\nTaken from Kaggle winner, but for a different problem. See, for instance, [this notebook](https://www.kaggle.com/code/haqishen/1st-place-soluiton-code-small-ver) (past Kaggle competition winner)","metadata":{}},{"cell_type":"code","source":"transforms_train = A.Compose([\n    A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=AUGMENTATION_PROBABILITY),\n    A.OneOf([\n        A.MotionBlur(blur_limit=5),\n        A.MedianBlur(blur_limit=5),\n        A.GaussianBlur(blur_limit=5),\n        A.GaussNoise(var_limit=(5.0, 30.0)),\n    ], p=AUGMENTATION_PROBABILITY),\n\n    A.OneOf([\n        A.OpticalDistortion(distort_limit=0.1),\n        A.GridDistortion(num_steps=5, distort_limit=0.1),\n        A.ElasticTransform(alpha=1),\n    ], p=AUGMENTATION_PROBABILITY),\n\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=AUGMENTATION_PROBABILITY),\n    A.CoarseDropout(max_holes=16, max_height=16, max_width=16, min_holes=1, min_height=8, min_width=8, p=AUGMENTATION_PROBABILITY),    \n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet stats\n    ToTensorV2()\n], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False))\n\ntransforms_validation = A.Compose([\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet stats\n    ToTensorV2()\n], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False))\n\nif DEBUG or not AUGMENTATION:\n    transforms_train = transforms_validation","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:20:43.003061Z","iopub.execute_input":"2024-10-02T09:20:43.003436Z","iopub.status.idle":"2024-10-02T09:20:43.024611Z","shell.execute_reply.started":"2024-10-02T09:20:43.003392Z","shell.execute_reply":"2024-10-02T09:20:43.023663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    def test_transformations():\n        # Create a dummy image and keypoints\n        dummy_image = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)\n        dummy_keypoints = [(100, 100), (150, 150)]\n\n        # Test training transformations\n        print(\"Testing training transformations:\")\n        for _ in range(5):  # Test multiple times due to randomness\n            transformed = transforms_train(image=dummy_image, keypoints=dummy_keypoints)\n            transformed_image = transformed['image']\n            transformed_keypoints = transformed['keypoints']\n\n            # Check if the image is not null and within expected range\n            assert transformed_image.numel() > 0, \"Transformed image is empty\"\n            assert torch.any(transformed_image != 0), \"Transformed image is all zeros\"\n            assert transformed_image.dtype == torch.float32, \"Transformed image is not float32\"\n            assert torch.all(transformed_image >= -3) and torch.all(transformed_image <= 3), f\"Transformed image values out of expected range: min={transformed_image.min()}, max={transformed_image.max()}\"\n\n            # Check keypoints\n            assert transformed_keypoints is not None, \"Transformation resulted in no keypoints\"\n            assert len(transformed_keypoints) == len(dummy_keypoints), \"Transformation changed number of keypoints\"\n            assert all(isinstance(kp, tuple) and len(kp) == 2 for kp in transformed_keypoints), \"Invalid keypoint format\"\n            assert all(0 <= kp[0] < 224 and 0 <= kp[1] < 224 for kp in transformed_keypoints), \"Keypoints out of image bounds\"\n\n            # Check image shape and type\n            assert transformed_image.shape == (3, 224, 224), f\"Unexpected shape: {transformed_image.shape}\"\n            assert isinstance(transformed_image, torch.Tensor), \"Transformed image is not a torch.Tensor\"\n\n            print(\"  Passed\")\n\n        # Test validation transformations\n        print(\"\\nTesting validation transformations:\")\n        transformed = transforms_validation(image=dummy_image, keypoints=dummy_keypoints)\n        transformed_image = transformed['image']\n        transformed_keypoints = transformed['keypoints']\n\n        # Check if the image is not null and within expected range\n        assert transformed_image.numel() > 0, \"Transformed image is empty\"\n        assert torch.any(transformed_image != 0), \"Transformed image is all zeros\"\n        assert transformed_image.dtype == torch.float32, \"Transformed image is not float32\"\n        assert torch.all(transformed_image >= -3) and torch.all(transformed_image <= 3), f\"Transformed image values out of expected range: min={transformed_image.min()}, max={transformed_image.max()}\"\n\n        # Check image shape and type\n        assert transformed_image.shape == (3, 224, 224), f\"Unexpected shape: {transformed_image.shape}\"\n        assert isinstance(transformed_image, torch.Tensor), \"Transformed image is not a torch.Tensor\"\n\n        # Check if keypoints are unchanged (validation should not modify keypoints)\n        assert transformed_keypoints == dummy_keypoints, \"Validation transformation modified keypoints\"\n\n        print(\"  Passed\")\n\n        print(\"\\nAll transformation tests passed!\")\n\n    # Run the test\n    test_transformations()","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:20:43.025859Z","iopub.execute_input":"2024-10-02T09:20:43.026110Z","iopub.status.idle":"2024-10-02T09:20:43.040107Z","shell.execute_reply.started":"2024-10-02T09:20:43.026081Z","shell.execute_reply":"2024-10-02T09:20:43.039225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    def test_multitask_dataset():\n        # Create a small subset of your data for testing\n        test_df = all_labels_df.head(10)  # Use first 10 rows for testing\n\n        # Initialize the dataset\n        test_dataset = MultiTaskDataset(\n            label_df=test_df,\n            series_descriptions=train_descriptions,\n            sagittal_t2_coordinates=sagittal_t2_coordinates,\n            sagittal_t1_coordinates=sagittal_t1_coordinates,\n            axial_t2_coordinates=axial_t2_coordinates,\n            phase='train',\n            transform=transforms_train\n        )\n\n        # Create a DataLoader\n        test_dataloader = DataLoader(\n            test_dataset,\n            batch_size=2,\n            shuffle=False,\n            collate_fn=custom_collate,\n            num_workers=0  # Use 0 for easier debugging\n        )\n\n        # Iterate through the dataloader\n        for batch in test_dataloader:\n            images = batch['images']\n            labels = batch['labels']\n            keypoints = batch['keypoints']\n            masks = batch['mask']\n            study_ids = batch['study_id']\n\n            print(f\"Batch size: {len(study_ids)}\")\n            print(f\"Study IDs: {study_ids}\")\n            print(f\"Labels shape: {labels.shape}\")\n            print(f\"Masks shape: {masks.shape}\")\n\n            # Check nested tensor structure\n            assert len([_ for _ in images.unbind()]) == 3, \"Nested tensor should have 3 elements\"\n            print(f\"Sagittal T2 shape: {images[0].shape}\")\n            print(f\"Sagittal T1 shape: {images[1].shape}\")\n            print(f\"Axial T2 shape: {images[2].shape}\")\n\n            # Visualize one image from each type\n            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n            # Function to normalize image for plotting\n            def normalize_for_plot(img):\n                return (img - img.min()) / (img.max() - img.min())\n\n            # Function to add rectangle patches for keypoints\n            def add_keypoint_patches_sagittal_t2(ax, kp, color='r', height = SAGITTAL_IMAGE_SHAPE[0], width = SAGITTAL_IMAGE_SHAPE[1]):\n                for level, points in kp.items():\n                    x, y = points[0]\n                    if 0 <= x < width and 0 <= y < height:\n                        rect = plt.Rectangle((x-5, y-5), 10, 10, fill=False, edgecolor=color)\n                        ax.add_patch(rect)\n                        ax.text(x, y-15, level, color=color, fontsize=8, ha='center')\n\n            # Plot Sagittal T2\n            axes[0].imshow(normalize_for_plot(images[0][0][0]).permute(1, 2, 0))\n            axes[0].set_title(\"Sagittal T2\")\n            add_keypoint_patches_sagittal_t2(axes[0], keypoints[0]['Sagittal T2'], height=images[0][0][0].shape[1], width=images[0][0][0].shape[2])\n\n            def add_keypoint_patches_sagittal_t1(ax, kp, color='r', height = SAGITTAL_IMAGE_SHAPE[0], width = SAGITTAL_IMAGE_SHAPE[1]):\n                for level, points in kp['right'].items():\n                    x, y = points[0]\n                    if 0 <= x < width and 0 <= y < height:\n                        rect = plt.Rectangle((x-5, y-5), 10, 10, fill=False, edgecolor=color)\n                        ax.add_patch(rect)\n                        ax.text(x, y-15, level, color=color, fontsize=8, ha='center')\n\n            # Plot Sagittal T1\n            axes[1].imshow(normalize_for_plot(images[1][0][0]).permute(1, 2, 0))\n            axes[1].set_title(\"Sagittal T1\")\n            add_keypoint_patches_sagittal_t1(axes[1], keypoints[0]['Sagittal T1'], height=images[1][0][0].shape[1], width=images[1][0][0].shape[2])\n\n            def add_keypoint_patches_axial_t2(ax, kp, color='r', height = AXIAL_IMAGE_SHAPE[0], width = AXIAL_IMAGE_SHAPE[1]):\n                for level, points in kp['L1/L2'].items():\n                    x, y = points[0]\n                    if 0 <= x < width and 0 <= y < height:\n                        rect = plt.Rectangle((x-5, y-5), 10, 10, fill=False, edgecolor=color)\n                        ax.add_patch(rect)\n                        ax.text(x, y-15, level, color=color, fontsize=8, ha='center')\n                    else:\n                        print(f\"Skipping keypoint {level} at {x}, {y} as it is out of bounds\")\n\n            # Plot Axial T2\n            axes[2].imshow(normalize_for_plot(images[2][0][0]).permute(1, 2, 0))\n            axes[2].set_title(\"Axial T2\")\n            add_keypoint_patches_axial_t2(axes[2], keypoints[0]['Axial T2'], height=images[2][0][0].shape[1], width=images[2][0][0].shape[2])\n\n            plt.tight_layout()\n            plt.show()\n\n            # Print keypoints for one study\n            print(\"Keypoints for first study:\")\n            for img_type, kp in keypoints[0].items():\n                print(f\"  {img_type}:\")\n                for level, points in kp.items():\n                    print(f\"    {level}: {points}\")\n\n            # Only process one batch for this test\n            break\n\n    if __name__ == \"__main__\":\n        test_multitask_dataset()","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:20:43.041520Z","iopub.execute_input":"2024-10-02T09:20:43.042159Z","iopub.status.idle":"2024-10-02T09:20:43.066173Z","shell.execute_reply.started":"2024-10-02T09:20:43.042115Z","shell.execute_reply":"2024-10-02T09:20:43.065280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Need to get rid of some transformations that completely distort the image\n### Unfortunately, Albumentations works on CPU so might slow everything down. Below is the implementation of transformations with torchvision, which are however limited in what they can do","metadata":{}},{"cell_type":"markdown","source":"# GPU Cropping","metadata":{}},{"cell_type":"code","source":"def gpu_crop_region(x, keypoints_to_use, crop_size=(64, 64), epoch=0, sr_model=None):\n    \"\"\"\n    Crop region around the predicted keypoint (x, y) on the GPU with optional super-resolution.\n    \n    x: Nested tensor containing images of different types\n    keypoints_to_use: List of dicts with keypoints for each image type\n    crop_size: (height, width) size of the crop\n    epoch: Current training epoch\n    sr_model: Super-resolution model (if applicable)\n    \n    Returns: Tuple of (Nested tensor of cropped images, Nested tensor of masks)\n    \"\"\"\n    h, w = crop_size\n    all_cropped_imgs = []\n    all_masks = []\n\n    # Get the device from the input tensor\n    device = x[0].device\n\n    for i, img_tensor in enumerate(x.unbind()):\n        if i == 0:  # Sagittal T2\n            img = img_tensor.squeeze(1)  # Remove the extra dimension\n            num_crops = 5\n            keypoint_order = [('Sagittal T2', level) for level in LEVELS]\n            img_h, img_w = SAGITTAL_IMAGE_SHAPE\n        elif i == 1:  # Sagittal T1\n            img = img_tensor\n            num_crops = 10\n            keypoint_order = [('Sagittal T1', side, level) for side in ['right', 'left'] for level in LEVELS]\n            img_h, img_w = SAGITTAL_IMAGE_SHAPE\n        else:  # Axial T2\n            img = img_tensor\n            num_crops = 10\n            keypoint_order = [('Axial T2', level, side) for level in LEVELS for side in ['right', 'left']]\n            img_h, img_w = AXIAL_IMAGE_SHAPE\n\n        batch_size = img.shape[0]\n        \n        # Prepare keypoints and masks\n        keypoints = torch.zeros(batch_size, num_crops, 2, device=device)\n        masks = torch.zeros(batch_size, num_crops, device=device)\n\n        for j in range(batch_size):\n            for idx, key in enumerate(keypoint_order):\n                if i == 0:  # Sagittal T2\n                    kp = keypoints_to_use[j][key[0]][key[1]]\n                elif i == 1:  # Sagittal T1\n                    kp = keypoints_to_use[j][key[0]][key[1]][key[2]]\n                else:  # Axial T2\n                    kp = keypoints_to_use[j][key[0]][key[1]][key[2]]\n                \n                if is_valid_keypoint(kp, img_h, img_w):\n                    keypoints[j, idx] = torch.tensor(kp, device=device)\n                    masks[j, idx] = 1.0\n                else:\n                    # If keypoint is invalid, set it to the center of the image\n                    keypoints[j, idx] = torch.tensor([img_w // 2, img_h // 2], device=device)\n                    masks[j, idx] = 0.0\n\n        # Crop images\n        cropped_imgs = crop_image(img, keypoints, crop_size)\n\n        # Set crops with invalid keypoints to zero\n        cropped_imgs[masks == 0] = 0\n\n        # Apply super-resolution if applicable\n        if sr_model:\n            with torch.no_grad():\n                if epoch > 15:\n                    output_size = (256, 256)\n                elif epoch > 10:\n                    output_size = (128, 128)\n                elif epoch > 5:\n                    output_size = (64, 64)\n                else:\n                    output_size = crop_size\n                \n                valid_crops = cropped_imgs[masks.bool()]\n                if valid_crops.numel() > 0:\n                    sr_crops = sr_model(valid_crops, output_size=output_size)\n                    sr_cropped_imgs = torch.zeros(batch_size, num_crops, 3, *output_size, device=device)\n                    sr_idx = 0\n                    for b in range(batch_size):\n                        for n in range(num_crops):\n                            if masks[b, n] == 1:\n                                sr_cropped_imgs[b, n] = sr_crops[sr_idx]\n                                sr_idx += 1\n                    cropped_imgs = sr_cropped_imgs\n\n        all_cropped_imgs.append(cropped_imgs)\n        all_masks.append(masks)\n    \n    # Return as nested tensors\n    return torch.nested.as_nested_tensor(all_cropped_imgs), torch.cat(all_masks, dim=1)\n\ndef is_valid_keypoint(kp, img_h, img_w):\n    \"\"\"\n    Check if a keypoint is valid (within image boundaries and not None).\n    \"\"\"\n    if isinstance(kp, (list, tuple)):\n        return (len(kp) == 2 and \n                0 <= kp[0] < img_w and \n                0 <= kp[1] < img_h)\n    elif isinstance(kp, torch.Tensor):\n        return (kp.numel() == 2 and \n                torch.all((0 <= kp) & (kp < torch.tensor([img_w, img_h], device=kp.device))))\n    else:\n        return False\n\ndef crop_image(img, keypoints, crop_size):\n    \"\"\"\n    GPU-friendly function to crop images based on keypoints.\n    \n    img: Tensor of shape (batch_size, num_images, channels, height, width) or (batch_size, channels, height, width)\n    keypoints: Tensor of shape (batch_size, num_crops, 2)\n    crop_size: Tuple (height, width) for the size of crops\n    \n    Returns: Tensor of cropped images (batch_size, num_crops, channels, crop_height, crop_width)\n    \"\"\"\n    batch_size, num_crops, _ = keypoints.shape\n    h, w = crop_size\n    \n    if img.dim() == 4:\n        img = img.unsqueeze(1)  # Add dimension for single image if not present\n    _, num_images, c, img_h, img_w = img.shape\n\n    # Compute crop boundaries\n    left = (keypoints[:, :, 0] - w // 2).long().clamp(min=0, max=img_w - w)\n    top = (keypoints[:, :, 1] - h // 2).long().clamp(min=0, max=img_h - h)\n    \n    # Create base indices for each crop\n    base_y = torch.arange(h, device=img.device).view(1, 1, -1, 1).expand(batch_size, num_crops, -1, w)\n    base_x = torch.arange(w, device=img.device).view(1, 1, 1, -1).expand(batch_size, num_crops, h, -1)\n    \n    # Offset indices by crop top-left corners\n    y_indices = (top.view(batch_size, num_crops, 1, 1) + base_y).clamp(max=img_h-1)\n    x_indices = (left.view(batch_size, num_crops, 1, 1) + base_x).clamp(max=img_w-1)\n    \n    # Reshape indices for gathering\n    b_indices = torch.arange(batch_size, device=img.device).view(-1, 1, 1, 1).expand(-1, num_crops, h, w)\n    i_indices = (torch.arange(num_crops, device=img.device) % num_images).view(1, -1, 1, 1).expand(batch_size, -1, h, w)\n    \n    # Gather crops\n    crops = img[b_indices, i_indices, :, y_indices, x_indices].permute(0, 1, 4, 2, 3)\n    \n    return crops","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:20:43.067433Z","iopub.execute_input":"2024-10-02T09:20:43.067716Z","iopub.status.idle":"2024-10-02T09:20:43.095301Z","shell.execute_reply.started":"2024-10-02T09:20:43.067685Z","shell.execute_reply":"2024-10-02T09:20:43.094292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    class TestGPUCropRegion(unittest.TestCase):\n        def setUp(self):\n            self.device = 'cpu'\n            self.batch_size = 2\n            self.crop_size = (64, 64)\n\n            # Create dummy nested tensor input\n            self.sagittal_t2 = torch.randn(self.batch_size, 1, 3, 224, 224, device=self.device)\n            self.sagittal_t1 = torch.randn(self.batch_size, 2, 3, 224, 224, device=self.device)\n            self.axial = torch.randn(self.batch_size, 5, 3, 128, 128, device=self.device)\n            self.dummy_input = torch.nested.nested_tensor([self.sagittal_t2, self.sagittal_t1, self.axial])\n\n            # Create dummy keypoints\n            self.keypoints_to_use = [\n                {\n                    'Sagittal T2': {level: [112, 112] for level in LEVELS},\n                    'Sagittal T1': {\n                        'right': {level: [112, 112] for level in LEVELS},\n                        'left': {level: [112, 112] for level in LEVELS}\n                    },\n                    'Axial T2': {level: {'right': [64, 64], 'left': [64, 64]} for level in LEVELS}\n                }\n                for _ in range(self.batch_size)\n            ]\n\n        def test_gpu_crop_region_output_shape(self):\n            cropped_images, crop_masks = gpu_crop_region(self.dummy_input, self.keypoints_to_use, self.crop_size)\n\n            self.assertEqual(len([_ for _ in cropped_images.unbind()]), 3, \"Should have 3 image types\")\n            self.assertEqual(cropped_images[0].shape, (self.batch_size, 5, 3, *self.crop_size), \"Incorrect shape for Sagittal T2\")\n            self.assertEqual(cropped_images[1].shape, (self.batch_size, 10, 3, *self.crop_size), \"Incorrect shape for Sagittal T1\")\n            self.assertEqual(cropped_images[2].shape, (self.batch_size, 10, 3, *self.crop_size), \"Incorrect shape for Axial T2\")\n\n            self.assertEqual(crop_masks.shape, (self.batch_size, N_LABELS), \"Incorrect mask shape for masks\")\n\n        def test_gpu_crop_region_mask_values(self):\n            _, crop_masks = gpu_crop_region(self.dummy_input, self.keypoints_to_use, self.crop_size)\n\n            for mask in crop_masks:\n                self.assertTrue(torch.all(mask == 1), \"All masks should be 1 when all keypoints are provided\")\n\n        def test_gpu_crop_region_missing_keypoints(self):\n            # Remove some keypoints\n            self.keypoints_to_use[0]['Sagittal T2']['L1/L2'] = []\n            self.keypoints_to_use[0]['Sagittal T1']['right']['L2/L3'] = []\n            self.keypoints_to_use[0]['Axial T2']['L3/L4']['left'] = []\n\n            _, crop_masks = gpu_crop_region(self.dummy_input, self.keypoints_to_use, self.crop_size)\n\n            self.assertEqual(crop_masks[0, 0].item(), 0, \"Mask should be 0 for missing Sagittal T2 keypoint\")\n            self.assertEqual(crop_masks[0, 5 + 1].item(), 0, \"Mask should be 0 for missing Sagittal T1 right keypoint\")\n            self.assertEqual(crop_masks[0, 15 + 5].item(), 0, \"Mask should be 0 for missing Axial T2 left keypoint\")\n\n        def test_crop_image_output(self):\n            # Test crop_image function directly\n            img = torch.randn(2, 1, 3, 224, 224, device=self.device)\n            keypoints = torch.tensor([[[112, 112]], [[56, 56]]], device=self.device)\n            crops = crop_image(img, keypoints, self.crop_size)\n\n            self.assertEqual(crops.shape, (2, 1, 3, *self.crop_size), \"Incorrect crop shape\")\n\n            # Check if the crop is centered around the keypoint\n            self.assertTrue(torch.allclose(crops[0, 0, :, 32, 32], img[0, 0, :, 112, 112], atol=1e-6))\n            self.assertTrue(torch.allclose(crops[1, 0, :, 32, 32], img[1, 0, :, 56, 56], atol=1e-6))\n\n        def test_gpu_crop_region_super_resolution(self):\n            # Mock super-resolution model\n            class MockSRModel(torch.nn.Module):\n                def forward(self, x, output_size):\n                    return F.interpolate(x, size=output_size, mode='bilinear', align_corners=False)\n\n            sr_model = MockSRModel()\n\n            for epoch in [0, 6, 11, 16]:\n                cropped_images, _ = gpu_crop_region(self.dummy_input, self.keypoints_to_use, self.crop_size, epoch=epoch, sr_model=sr_model)\n\n                if epoch > 15:\n                    expected_size = (256, 256)\n                elif epoch > 10:\n                    expected_size = (128, 128)\n                elif epoch > 5:\n                    expected_size = (64, 64)\n                else:\n                    expected_size = self.crop_size\n\n                for img_type in cropped_images:\n                    self.assertEqual(img_type.shape[-2:], expected_size, f\"Incorrect super-resolution size for epoch {epoch}\")\n\n        def test_gpu_crop_region_preserves_gradients(self):\n            # Create input tensors that require gradients\n            sagittal_t2 = torch.randn(self.batch_size, 1, 3, 224, 224, device=self.device, requires_grad=True)\n            sagittal_t1 = torch.randn(self.batch_size, 2, 3, 224, 224, device=self.device, requires_grad=True)\n            axial = torch.randn(self.batch_size, 5, 3, 128, 128, device=self.device, requires_grad=True)\n            dummy_input = torch.nested.as_nested_tensor([sagittal_t2, sagittal_t1, axial])\n\n            # Perform the crop operation\n            cropped_images, _ = gpu_crop_region(dummy_input, self.keypoints_to_use, self.crop_size)\n\n            # Check if cropped images require gradients\n            self.assertTrue(cropped_images[0].requires_grad, \"Cropped Sagittal T2 images should require gradients\")\n            self.assertTrue(cropped_images[1].requires_grad, \"Cropped Sagittal T1 images should require gradients\")\n            self.assertTrue(cropped_images[2].requires_grad, \"Cropped Axial T2 images should require gradients\")\n\n            # Compute a dummy loss and perform backward pass\n            dummy_loss = cropped_images[0].sum() + cropped_images[1].sum() + cropped_images[2].sum()\n            dummy_loss.backward()\n\n            # Check if gradients are computed for input tensors\n            self.assertIsNotNone(sagittal_t2.grad, \"Sagittal T2 input should have gradients\")\n            self.assertIsNotNone(sagittal_t1.grad, \"Sagittal T1 input should have gradients\")\n            self.assertIsNotNone(axial.grad, \"Axial T2 input should have gradients\")\n\n            # Check if gradients are non-zero\n            self.assertGreater(sagittal_t2.grad.abs().sum().item(), 0, \"Sagittal T2 gradients should be non-zero\")\n            self.assertGreater(sagittal_t1.grad.abs().sum().item(), 0, \"Sagittal T1 gradients should be non-zero\")\n            self.assertGreater(axial.grad.abs().sum().item(), 0, \"Axial T2 gradients should be non-zero\")\n\n    unittest.main(argv=[''], exit=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:20:43.096697Z","iopub.execute_input":"2024-10-02T09:20:43.096978Z","iopub.status.idle":"2024-10-02T09:20:43.127118Z","shell.execute_reply.started":"2024-10-02T09:20:43.096948Z","shell.execute_reply":"2024-10-02T09:20:43.126311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class MultiTaskModel(nn.Module):\n    def __init__(self, backbone_name=MODEL_NAME, sagittal_t2_num_classes=5, sagittal_t1_num_classes=10, sagittal_num_keypoints=5, axial_num_classes=10, axial_num_keypoints=2):\n        super(MultiTaskModel, self).__init__()\n        # Load a timm backbone\n        self.sagittal_backbone = timm.create_model(backbone_name, pretrained=True, num_classes=0, global_pool='')\n        self.axial_backbone = timm.create_model(backbone_name, pretrained=True, num_classes=0, global_pool='')\n        self.sagittal_t2_cropped_backbone = timm.create_model('efficientnet_b1', pretrained=True, num_classes=0, global_pool='')\n        self.sagittal_t1_cropped_backbone = timm.create_model('efficientnet_b1', pretrained=True, num_classes=0, global_pool='')\n        self.axial_cropped_backbone = timm.create_model('efficientnet_b1', pretrained=True, num_classes=0, global_pool='')\n\n        # Ensure all backbone parameters require gradients\n        for param in self.sagittal_backbone.parameters():\n            param.requires_grad = True\n        for param in self.axial_backbone.parameters():\n            param.requires_grad = True\n\n        # Get the number of features from the last layer of the backbone\n        self.num_features = self.sagittal_backbone.feature_info[-1]['num_chs']\n\n        self.num_features_cropped = self.sagittal_t2_cropped_backbone.num_features\n\n        # Add adaptive pooling\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n\n        # Keypoint prediction branches\n        self.sagittal_t2_keypoint_fc = nn.Sequential(\n            nn.Linear(self.num_features, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, sagittal_num_keypoints * 2)\n        )\n\n        self.sagittal_t1_keypoint_fc = nn.Sequential(\n            nn.Linear(self.num_features, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, sagittal_num_keypoints * 2)\n        )\n\n        self.axial_t2_keypoint_fc = nn.Sequential(\n            nn.Linear(self.num_features, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, axial_num_keypoints * 2)\n        )\n\n        self.sagittal_t2_fc_cropped = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(self.num_features_cropped, 512),\n                nn.BatchNorm1d(512),\n                nn.ReLU(),\n                nn.Dropout(0.3),\n                nn.Linear(512, 256),\n                nn.BatchNorm1d(256),\n                nn.ReLU(),\n                nn.Dropout(0.3),\n                nn.Linear(256, 3),\n            ) for _ in range(sagittal_t2_num_classes)\n        ])\n\n        self.sagittal_t1_fc_cropped = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(self.num_features_cropped, 512),\n                nn.BatchNorm1d(512),\n                nn.ReLU(),\n                nn.Dropout(0.3),\n                nn.Linear(512, 256),\n                nn.BatchNorm1d(256),\n                nn.ReLU(),\n                nn.Dropout(0.3),\n                nn.Linear(256, 3),\n            ) for _ in range(sagittal_t1_num_classes)\n        ])\n        \n        self.axial_fc_cropped = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(self.num_features_cropped, 512),\n                nn.BatchNorm1d(512),\n                nn.ReLU(),\n                nn.Dropout(0.3),\n                nn.Linear(512, 256),\n                nn.BatchNorm1d(256),\n                nn.ReLU(),\n                nn.Dropout(0.3),\n                nn.Linear(256, 3),\n            ) for _ in range(axial_num_classes)\n        ])\n\n    \n    def forward(self, x, task='classification'):\n        batch_size = x[0].size(0)  # All nested tensors have the same batch size\n\n        if task == 'localization':\n            # Process Sagittal T2 images (first nested tensor)\n            sagittal_t2_features = self.sagittal_backbone(x[0].squeeze(1))  # Remove the extra dimension\n            sagittal_t2_features = self.adaptive_pool(sagittal_t2_features).squeeze(-1).squeeze(-1)\n            \n            # Process Sagittal T1 images (second nested tensor)\n            sagittal_t1_features = self.sagittal_backbone(x[1].view(-1, *x[1].shape[2:]))\n            sagittal_t1_features = self.adaptive_pool(sagittal_t1_features).squeeze(-1).squeeze(-1)\n\n            # Process Axial images (third nested tensor)\n            axial_features = self.axial_backbone(x[2].view(-1, *x[2].shape[2:]))\n            axial_features = self.adaptive_pool(axial_features).squeeze(-1).squeeze(-1)\n\n            # Sagittal T2 localization\n            sagittal_t2_keypoints = self.sagittal_t2_keypoint_fc(sagittal_t2_features).view(batch_size, 1, 5, 2)\n\n            # Sagittal T1 localization\n            sagittal_t1_keypoints = self.sagittal_t1_keypoint_fc(sagittal_t1_features).view(batch_size, 2, 5, 2)\n\n            # Axial localization\n            axial_keypoints = self.axial_t2_keypoint_fc(axial_features).view(batch_size, 5, 2, 2)\n\n            nested_keypoints = torch.nested.as_nested_tensor([sagittal_t2_keypoints, sagittal_t1_keypoints, axial_keypoints])\n            return nested_keypoints\n\n        elif task == 'classification':\n            # Process Sagittal T2 images (first nested tensor)\n            sagittal_t2_features = self.sagittal_t2_cropped_backbone(x[0].view(-1, *x[0].shape[2:]))\n            sagittal_t2_features = self.adaptive_pool(sagittal_t2_features).squeeze(-1).squeeze(-1)\n\n            # Process Sagittal T1 images (second nested tensor)\n            sagittal_t1_features = self.sagittal_t1_cropped_backbone(x[1].view(-1, *x[1].shape[2:]))\n            sagittal_t1_features = self.adaptive_pool(sagittal_t1_features).squeeze(-1).squeeze(-1)\n\n            # Process Axial images (third nested tensor)\n            axial_features = self.axial_cropped_backbone(x[2].view(-1, *x[2].shape[2:]))\n            axial_features = self.adaptive_pool(axial_features).squeeze(-1).squeeze(-1)\n\n            # Process Sagittal T2 (5 cropped images)\n            sagittal_t2_unflattened = sagittal_t2_features.view(batch_size, 5, -1)\n            sagittal_t2_outputs = torch.stack([fc(sagittal_t2_unflattened[:, i, :]) for i, fc in enumerate(self.sagittal_t2_fc_cropped)], dim=1)\n\n            # Process Sagittal T1 (10 cropped images)\n            sagittal_t1_unflattened = sagittal_t1_features.view(batch_size, 10, -1)\n            sagittal_t1_outputs = torch.stack([fc(sagittal_t1_unflattened[:, i, :]) for i, fc in enumerate(self.sagittal_t1_fc_cropped)], dim=1)\n\n            # Process Axial (10 cropped images)\n            axial_unflattened = axial_features.view(batch_size, 10, -1)\n            axial_outputs = torch.stack([fc(axial_unflattened[:, i, :]) for i, fc in enumerate(self.axial_fc_cropped)], dim=1)\n\n            return torch.cat([sagittal_t2_outputs, sagittal_t1_outputs, axial_outputs], dim=1)\n\n        else:\n            raise ValueError(f\"Invalid task: {task}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:25:33.098150Z","iopub.execute_input":"2024-10-02T09:25:33.098734Z","iopub.status.idle":"2024-10-02T09:25:33.131495Z","shell.execute_reply.started":"2024-10-02T09:25:33.098691Z","shell.execute_reply":"2024-10-02T09:25:33.130401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing the model","metadata":{}},{"cell_type":"code","source":"if DEBUG:\n    class TestMultiTaskModel(unittest.TestCase):\n        def setUp(self):\n            self.model = MultiTaskModel(\n                backbone_name='densenet161.tv_in1k',\n                sagittal_t2_num_classes=5,\n                sagittal_t1_num_classes=10,\n                sagittal_num_keypoints=5,\n                axial_num_classes=10,\n                axial_num_keypoints=2\n            )\n            self.batch_size = 2\n            self.crop_size = (64, 64)\n\n            # Create dummy nested tensor input for localization task\n            sagittal_t2 = torch.randn(self.batch_size, 1, 3, 490, 275)  # (B, 1, C, H, W)\n            sagittal_t1 = torch.randn(self.batch_size, 2, 3, 490, 275)  # (B, 2, C, H, W)\n            axial = torch.randn(self.batch_size, 5, 3, 310, 250)        # (B, 5, C, H, W)\n            self.dummy_input_localization = torch.nested.nested_tensor([sagittal_t2, sagittal_t1, axial])\n\n            # Create dummy keypoints for classification task\n            self.keypoints_to_use = [\n                {\n                    'Sagittal T2': {level: [112, 112] for level in LEVELS},\n                    'Sagittal T1': {\n                        'right': {level: [112, 112] for level in LEVELS},\n                        'left': {level: [112, 112] for level in LEVELS}\n                    },\n                    'Axial T2': {level: {'right': [64, 64], 'left': [64, 64]} for level in LEVELS}\n                }\n                for _ in range(self.batch_size)\n            ]\n\n            # Create dummy nested tensor input for classification task\n            self.dummy_input_classification, _ = gpu_crop_region(self.dummy_input_localization, self.keypoints_to_use, self.crop_size)\n\n        def test_model_structure(self):\n            self.assertIsInstance(self.model.sagittal_backbone, nn.Module)\n            self.assertIsInstance(self.model.axial_backbone, nn.Module)\n            self.assertIsInstance(self.model.sagittal_t2_keypoint_fc, nn.Linear)\n            self.assertIsInstance(self.model.sagittal_t1_keypoint_fc, nn.Linear)\n            self.assertIsInstance(self.model.axial_t2_keypoint_fc, nn.Linear)\n            self.assertEqual(len(self.model.sagittal_t2_fc_cropped), 5)\n            self.assertEqual(len(self.model.sagittal_t1_fc_cropped), 10)\n            self.assertEqual(len(self.model.axial_fc_cropped), 10)\n\n        def test_localization_task(self):\n            self.model.eval()\n            with torch.no_grad():\n                output = self.model(self.dummy_input_localization, task='localization')\n\n            self.assertIsInstance(output, torch.Tensor)\n            self.assertEqual(len([_ for _ in output.unbind()]), 3)  # Should have 3 image types\n            self.assertEqual(output[0].squeeze().shape, (self.batch_size, 5, 2))\n            self.assertEqual(output[1].shape, (self.batch_size, 2, 5, 2))\n            self.assertEqual(output[2].shape, (self.batch_size, 5, 2, 2))\n\n        def test_classification_task(self):\n            self.model.eval()\n            with torch.no_grad():\n                output = self.model(self.dummy_input_classification, task='classification')\n\n            self.assertIsInstance(output, torch.Tensor)\n\n            self.assertEqual(output.shape, (self.batch_size, N_LABELS, 3))\n\n        def test_invalid_task(self):\n            with self.assertRaises(ValueError):\n                self.model(self.dummy_input_localization, task='invalid_task')\n\n        def test_model_trainable_parameters(self):\n            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n            total_params = sum(p.numel() for p in self.model.parameters())\n\n            self.assertGreater(trainable_params, 0, \"Model should have trainable parameters\")\n            self.assertEqual(trainable_params, total_params, \"All parameters should be trainable\")\n\n            # If not all parameters are trainable, print out which ones are not\n            if trainable_params != total_params:\n                for name, param in self.model.named_parameters():\n                    if not param.requires_grad:\n                        print(f\"Parameter {name} is not trainable\")\n        def test_all_parameters_require_grad(self):\n            all_require_grad = all(p.requires_grad for p in self.model.parameters())\n            self.assertTrue(all_require_grad, \"Not all parameters require gradients\")\n\n            # Print out which parameters don't require gradients, if any\n            if not all_require_grad:\n                for name, param in self.model.named_parameters():\n                    if not param.requires_grad:\n                        print(f\"Parameter {name} does not require gradients\")\n\n        def test_model_output_range(self):\n            self.model.eval()\n            with torch.no_grad():\n                loc_output = self.model(self.dummy_input_localization, task='localization')\n                class_output = self.model(self.dummy_input_classification, task='classification')\n\n            # Check if keypoints are within image dimensions\n            self.assertTrue(torch.all(loc_output[0] >= 0))\n            self.assertTrue(torch.all(loc_output[0] <= 224))  # Assuming 224x224 input\n\n            # Check if classification outputs are valid probabilities\n            self.assertTrue(torch.all(class_output >= 0))\n            self.assertTrue(torch.all(class_output <= 1))\n\n        def test_backbone_freeze_unfreeze(self):\n            # Test freezing backbone\n            for param in self.model.sagittal_backbone.parameters():\n                param.requires_grad = False\n\n            self.assertTrue(all(not p.requires_grad for p in self.model.sagittal_backbone.parameters()))\n\n            # Test unfreezing backbone\n            for param in self.model.sagittal_backbone.parameters():\n                param.requires_grad = True\n\n            self.assertTrue(all(p.requires_grad for p in self.model.sagittal_backbone.parameters()))\n\n        def test_forward_pass_with_grad(self):\n            self.model.train()\n            # Ensure input requires grad\n            self.dummy_input_localization = torch.nested.nested_tensor([t.requires_grad_() for t in self.dummy_input_localization.unbind()])\n            output = self.model(self.dummy_input_localization, task='localization')\n\n            # Sum all outputs\n            loss = sum(o.sum() for o in output)\n\n            # Print debug information\n            print(f\"Loss: {loss}\")\n            print(f\"Loss requires grad: {loss.requires_grad}\")\n\n            for i, o in enumerate(output):\n                print(f\"Output {i} requires grad: {o.requires_grad}\")\n                print(f\"Output {i} grad_fn: {o.grad_fn}\")\n\n            # Try to backward\n            try:\n                    loss.backward()\n            except Exception as e:\n                print(f\"Error during backward pass: {e}\")\n                for i, o in enumerate(output):\n                    print(f\"Output {i} grad_fn after error: {o.grad_fn}\")\n                raise\n\n            # Check gradients for all parameters\n            for name, param in self.model.named_parameters():\n                if param.requires_grad:\n                    if param.grad is None:\n                        print(f\"Gradient is None for parameter: {name}\")\n                    elif torch.sum(param.grad) == 0:\n                        print(f\"Gradient is all zeros for parameter: {name}\")\n                else:\n                    print(f\"Parameter does not require grad: {name}\")\n\n            # Assert that at least some parameters have non-zero gradients\n            grads = [param.grad for param in self.model.parameters() if param.requires_grad]\n            self.assertTrue(any(grad is not None and torch.sum(grad) != 0 for grad in grads), \n                            \"No parameter has non-zero gradient\")\n            \n\n        def test_forward_pass_with_grad_classification(self):\n            self.model.train()\n            # Ensure input requires grad\n            self.dummy_input_classification = torch.nested.nested_tensor([t.requires_grad_() for t in self.dummy_input_classification.unbind()])\n            output = self.model(self.dummy_input_classification, task='classification')\n\n            # Create dummy labels\n            dummy_labels = torch.randint(0, 3, (self.batch_size, N_LABELS)).to(output.device)\n\n            # Use CrossEntropyLoss\n            criterion = nn.CrossEntropyLoss()\n            loss = criterion(output.view(-1, 3), dummy_labels.view(-1))\n\n            # Print debug information\n            print(f\"Classification Loss: {loss}\")\n            print(f\"Loss requires grad: {loss.requires_grad}\")\n            print(f\"Output requires grad: {output.requires_grad}\")\n            print(f\"Output grad_fn: {output.grad_fn}\")\n\n            # Try to backward\n            try:\n                loss.backward()\n            except Exception as e:\n                print(f\"Error during backward pass: {e}\")\n                print(f\"Output grad_fn after error: {output.grad_fn}\")\n                raise\n\n            # Check gradients for all parameters\n            for name, param in self.model.named_parameters():\n                if param.requires_grad:\n                    if param.grad is None:\n                        print(f\"Gradient is None for parameter: {name}\")\n                    elif torch.sum(param.grad) == 0:\n                        print(f\"Gradient is all zeros for parameter: {name}\")\n                else:\n                    print(f\"Parameter does not require grad: {name}\")\n\n            # Assert that at least some parameters have non-zero gradients\n            grads = [param.grad for param in self.model.parameters() if param.requires_grad]\n            self.assertTrue(any(grad is not None and torch.sum(grad) != 0 for grad in grads), \n                            \"No parameter has non-zero gradient\")\n\n    # Run the tests\n    unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestMultiTaskModel))","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:20:43.161669Z","iopub.execute_input":"2024-10-02T09:20:43.161978Z","iopub.status.idle":"2024-10-02T09:20:43.200445Z","shell.execute_reply.started":"2024-10-02T09:20:43.161947Z","shell.execute_reply":"2024-10-02T09:20:43.199641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"## Helper functions","metadata":{}},{"cell_type":"markdown","source":"### To possibly not evaluate models with missing labels at the beginning of training","metadata":{}},{"cell_type":"markdown","source":"## Actual Training Loop","metadata":{}},{"cell_type":"markdown","source":"### Would be good to implement stratified sampling","metadata":{}},{"cell_type":"code","source":"def points_to_tensor(points):\n    if not points:  # If points is []\n        return torch.tensor([]).to(device)\n    else:  # If points is [[x,y]]\n        return torch.tensor(points, dtype=torch.float32).squeeze(0).to(device) ","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:20:43.201541Z","iopub.execute_input":"2024-10-02T09:20:43.201843Z","iopub.status.idle":"2024-10-02T09:20:43.212768Z","shell.execute_reply.started":"2024-10-02T09:20:43.201801Z","shell.execute_reply":"2024-10-02T09:20:43.211995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomClassifierCriterion(nn.Module):\n    def __init__(self, criterion = nn.CrossEntropyLoss()):\n        super(CustomClassifierCriterion, self).__init__()\n        self.criterion = criterion\n        self.sagittal_t2_indices = range(0, 5)\n        self.sagittal_t1_indices = range(5, 15)\n        self.axial_t2_indices = range(15, 25)\n\n    def forward(self, predictions, labels):\n        device = predictions.device\n        \n        if not isinstance(labels, torch.Tensor):\n            labels = torch.tensor(labels.values, dtype=torch.long, device=device)\n        else:\n            labels = labels.to(device)\n        \n        total_loss = 0\n        sagittal_t2_loss = 0\n        sagittal_t1_loss = 0\n        axial_t2_loss = 0\n        for idx in range(N_LABELS):\n            prediction = predictions[:, idx, :]\n            label = labels[:, idx]\n            loss = self.criterion(prediction, label) / N_LABELS\n            total_loss += loss\n            if idx in self.sagittal_t2_indices:\n                sagittal_t2_loss += loss.item()\n            elif idx in self.sagittal_t1_indices:\n                sagittal_t1_loss += loss.item()\n            elif idx in self.axial_t2_indices:\n                axial_t2_loss += loss.item()\n\n        return total_loss, {\n            'Sagittal T2': sagittal_t2_loss,\n            'Sagittal T1': sagittal_t1_loss,\n            'Axial T2': axial_t2_loss\n        }","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:20:43.213810Z","iopub.execute_input":"2024-10-02T09:20:43.214094Z","iopub.status.idle":"2024-10-02T09:20:43.225287Z","shell.execute_reply.started":"2024-10-02T09:20:43.214063Z","shell.execute_reply":"2024-10-02T09:20:43.224494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_keypoints(model, images, true_keypoints, device, epoch, fold, output_dir, study_id):\n    model.eval()\n    \n    images = images.clone().detach().to(device)\n    x = []\n    for tensor in images.unbind():\n        x.append(tensor[0].unsqueeze(dim = 0))\n    \n    with torch.no_grad():\n        predicted_keypoints = model(x, task='localization')\n    \n    # Unpack the nested tensor\n    sagittal_t2_keypoints, sagittal_t1_keypoints, axial_keypoints = predicted_keypoints.unbind()\n    \n    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n    image_types = ['Sagittal T2', 'Sagittal T1', 'Axial T2']\n    \n    for idx, (ax, image_type) in enumerate(zip(axs, image_types)):\n        if image_type == 'Sagittal T2':\n            img = x[0].squeeze().squeeze().cpu()\n            pred_kp = sagittal_t2_keypoints[0, 0].cpu()\n            true_kp = true_keypoints['Sagittal T2']\n        elif image_type == 'Sagittal T1':\n            img = x[1][0, 0, ...].cpu()\n            pred_kp = sagittal_t1_keypoints[0, 0].cpu()  # Assuming right side\n            true_kp = true_keypoints['Sagittal T1']['right']\n        else:  # Axial T2\n            img = x[2][0, 0, ...].cpu()  # Assuming the first axial slice\n            pred_kp = axial_keypoints[0, 0].cpu()  # Assuming first level\n            true_kp = true_keypoints['Axial T2']['L1/L2']\n        \n        img_plot = img.clone().detach()\n        ax.imshow(img_plot.permute(1, 2, 0))\n        ax.set_title(image_type)\n        \n        # Plot predicted keypoints\n        for kp in pred_kp:\n            rect = patches.Rectangle((kp[0]-5, kp[1]-5), 10, 10, linewidth=1, edgecolor='r', facecolor='none')\n            ax.add_patch(rect)\n        \n        # Plot true keypoints\n        for kp in true_kp.values():\n            kp = kp[0]\n            if kp:  # Check if keypoint exists\n                rect = patches.Rectangle((kp[0]-5, kp[1]-5), 10, 10, linewidth=1, edgecolor='g', facecolor='none')\n                ax.add_patch(rect)\n    \n    plt.tight_layout()\n    plt.savefig(f'{output_dir}/keypoint_visualization_study_{study_id}_epoch_{epoch}_fold-{fold}.png')\n    plt.close()\n\ndef save_crops_for_study(cropped_imgs, study_id, epoch, output_dir):\n    os.makedirs(output_dir, exist_ok=True)\n    image_types = ['Sagittal_T2', 'Sagittal_T1', 'Axial_T2']\n    \n    for i, img_type in enumerate(image_types):\n        crops = cropped_imgs[i]\n        filename = f'{output_dir}/{img_type}_study_{study_id}_epoch_{epoch}.png'\n        grid = vutils.make_grid(crops[0], nrow=crops.shape[1], padding=2, normalize=True)\n        vutils.save_image(grid, filename)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:20:43.226408Z","iopub.execute_input":"2024-10-02T09:20:43.226716Z","iopub.status.idle":"2024-10-02T09:20:43.242337Z","shell.execute_reply.started":"2024-10-02T09:20:43.226686Z","shell.execute_reply":"2024-10-02T09:20:43.241468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.nn.utils import clip_grad_norm_\n\ndef clip_and_count(parameters, max_norm, norm_type=2):\n    total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type) for p in parameters if p.grad is not None]), norm_type)\n    clip_coef = max_norm / (total_norm + 1e-6)\n    clip_coef_clamped = torch.clamp(clip_coef, max=1.0)\n    for p in parameters:\n        if p.grad is not None:\n            p.grad.detach().mul_(clip_coef_clamped.to(p.grad.device))\n    return total_norm > max_norm","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:24:15.649686Z","iopub.execute_input":"2024-10-02T09:24:15.650481Z","iopub.status.idle":"2024-10-02T09:24:15.657243Z","shell.execute_reply.started":"2024-10-02T09:24:15.650439Z","shell.execute_reply":"2024-10-02T09:24:15.656188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"autocast = torch.amp.autocast(enabled=USE_AUTOMATIC_MIXED_PRECISION, dtype= torch.float16, device_type = device)\nlocalizer_scaler = torch.amp.GradScaler(enabled=USE_AUTOMATIC_MIXED_PRECISION, init_scale=4096)\nclassifier_scaler = torch.amp.GradScaler(enabled=USE_AUTOMATIC_MIXED_PRECISION, init_scale=4096)\n\nkfold = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\nfor fold, (training_index, validation_index) in enumerate(kfold.split(range(len(train_df)))):\n    print('#' * 30)\n    print(f'Starting fold {fold + 1}')\n    print('#' * 30)\n\n    current_training_index = train_df[train_df['study_id'].isin(all_labels_df['study_id'])].index.intersection(training_index)\n    current_validation_index = train_df[train_df['study_id'].isin(all_labels_df['study_id'])].index.intersection(validation_index)\n    study_id_for_kp_plotting = train_df.iloc[current_validation_index[0]]['study_id']\n    training_rows = train_df.iloc[current_training_index]\n    validation_rows = train_df.iloc[current_validation_index]\n\n    print('Training length: ', len(training_rows), 'Validation length: ', len(validation_rows))\n\n    training_dataset = MultiTaskDataset(label_df=training_rows, phase='train', transform=transforms_train)\n    training_dataloader = DataLoader(\n        training_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        pin_memory=True,\n        drop_last=True,\n        num_workers=N_WORKERS,\n        collate_fn=custom_collate\n    )\n\n    validation_dataset = MultiTaskDataset(label_df=validation_rows, phase='val', transform=transforms_validation)\n    validation_dataloader = DataLoader(\n        validation_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=False,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=N_WORKERS,\n        collate_fn=custom_collate\n    )\n    \n    model = MultiTaskModel()\n    model.to(device)\n    model.train()\n    \n    keypoint_params = list(model.sagittal_backbone.parameters()) + \\\n                      list(model.axial_backbone.parameters()) + \\\n                      list(model.sagittal_t2_keypoint_fc.parameters()) + \\\n                      list(model.sagittal_t1_keypoint_fc.parameters()) + \\\n                      list(model.axial_t2_keypoint_fc.parameters())\n\n    classification_params = list(model.sagittal_t2_cropped_backbone.parameters()) + \\\n                            list(model.sagittal_t1_cropped_backbone.parameters()) + \\\n                            list(model.axial_cropped_backbone.parameters()) + \\\n                            [p for fc_list in [model.sagittal_t2_fc_cropped, \n                                               model.sagittal_t1_fc_cropped, \n                                               model.axial_fc_cropped] \n                             for fc in fc_list for p in fc.parameters()]\n    \n    keypoint_optimizer = AdamW(keypoint_params, lr=1e-4, weight_decay = WEIGHT_DECAY)\n    classification_optimizer = AdamW(classification_params, lr=1e-4, weight_decay = WEIGHT_DECAY)\n\n    # Separate schedulers\n    keypoint_scheduler = ReduceLROnPlateau(keypoint_optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n    classification_scheduler = ReduceLROnPlateau(classification_optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n    \n    localizer_criterion = nn.MSELoss()\n    \n    weights = torch.tensor([1.0, 2.0, 4.0])\n    classifier_criterion = CustomClassifierCriterion(nn.CrossEntropyLoss(weight=weights.to(device)))\n    \n    best_loss = float('inf')\n    best_wall = float('inf')\n    es_step = 0\n    \n    # New variables for tracking and checkpoints\n    localizer_performance_threshold = 1.2 * 10e5\n    localizer_freeze_threshold = 5.0 * 10e3\n    full_dataset_used = False\n    localizer_frozen = False\n    \n    # For plotting\n    epoch_metrics = {\n        'train_loss': [], 'val_loss': [], 'val_wall': [], \n        'train_class_loss': [], 'train_loc_loss': [], 'val_class_loss': [], 'val_loc_loss': [], \n        'train_loc_losses': {'Sagittal T2': [], 'Sagittal T1': [], 'Axial T2': []},\n        'train_class_losses': {'Sagittal T2': [], 'Sagittal T1': [], 'Axial T2': []},\n        'val_loc_losses': {'Sagittal T2': [], 'Sagittal T1': [], 'Axial T2': []},\n        'val_class_losses': {'Sagittal T2': [], 'Sagittal T1': [], 'Axial T2': []}\n    }\n    \n    keypoint_clip_count = 0\n    classification_clip_count = 0\n    \n    for epoch in tqdm(range(1, EPOCHS + 1), desc=\"Epochs\"):\n        print(f'Starting epoch {epoch}')\n\n        # Training phase\n        model.train()\n        total_class_loss = 0\n        total_loc_loss = 0\n        loc_losses = {'Sagittal T2': 0, 'Sagittal T1': 0, 'Axial T2': 0}\n        class_losses = {'Sagittal T2': 0, 'Sagittal T1': 0, 'Axial T2': 0}\n        \n        with tqdm(training_dataloader, leave=False, desc=\"Training\") as loaded_items:\n            keypoint_optimizer.zero_grad()\n            classification_optimizer.zero_grad()\n            for idx, batch in enumerate(loaded_items):\n                images = batch['images'].to(device)\n                keypoint_labels = batch['keypoints']\n                class_labels = batch['labels'].to(device)\n                mask = batch['mask'].to(device)\n\n                with autocast:\n                    # Step 1: Localization\n                    with torch.set_grad_enabled(not localizer_frozen):\n                        predicted_keypoints = model(images, task='localization')\n                    \n                    # Unpack the nested tensor\n                    sagittal_t2_keypoints, sagittal_t1_keypoints, axial_keypoints = predicted_keypoints.unbind()\n                    sagittal_t2_keypoints = sagittal_t2_keypoints.squeeze()\n                    \n                    # Step 2: Determine which keypoints to use and calculate loss\n                    keypoints_to_use = []\n                    loc_loss = 0\n\n                    for batch_idx, batch_item in enumerate(keypoint_labels):\n                        sample_keypoints = {}\n                        \n                        for image_type, keypoints in batch_item.items():\n                            sample_keypoints[image_type] = {}\n                            \n                            if image_type == 'Sagittal T2':\n                                for level_idx, level in enumerate(LEVELS):\n                                    points = keypoints.get(level, [])\n                                    if mask[batch_idx, 0] == 0:\n                                        sample_keypoints[image_type][level] = []\n                                    else:\n                                        sample_keypoints[image_type][level] = points_to_tensor(points) if points else sagittal_t2_keypoints[batch_idx, level_idx].detach()\n                            \n                            elif image_type == 'Sagittal T1':\n                                sample_keypoints[image_type] = {'right': {}, 'left': {}}\n                                for side_idx, side in enumerate(['right', 'left']):\n                                    mask_idx = 1 if side == 'right' else 2\n                                    for level_idx, level in enumerate(LEVELS):\n                                        points = keypoints[side].get(level, [])\n                                        if mask[batch_idx, mask_idx] == 0:\n                                            sample_keypoints[image_type][side][level] = []\n                                        else:\n                                            sample_keypoints[image_type][side][level] = points_to_tensor(points) if points else sagittal_t1_keypoints[batch_idx, side_idx, level_idx].detach()\n                            \n                            elif image_type == 'Axial T2':\n                                for level_idx, level in enumerate(LEVELS):\n                                    sides = keypoints.get(level, {})\n                                    mask_idx = 3 + level_idx\n                                    sample_keypoints[image_type][level] = {}\n                                    for side_idx, side in enumerate(['right', 'left']):\n                                        points = sides.get(side, [])\n                                        if mask[batch_idx, mask_idx] == 0:\n                                            sample_keypoints[image_type][level][side] = []\n                                        else:\n                                            sample_keypoints[image_type][level][side] = points_to_tensor(points) if points else axial_keypoints[batch_idx, level_idx, side_idx].detach()\n                        \n                        keypoints_to_use.append(sample_keypoints)\n\n                    if not localizer_frozen:\n                        for batch_idx, sample_keypoints in enumerate(keypoints_to_use):\n                            for image_type, keypoints in sample_keypoints.items():\n                                if image_type == 'Sagittal T2':\n                                    for level_idx, level in enumerate(LEVELS):\n                                        points = keypoints.get(level, [])\n                                        if len(points) > 0:\n                                            level_loss = localizer_criterion(\n                                                sagittal_t2_keypoints[batch_idx, level_idx],\n                                                points\n                                            ) * mask[batch_idx, 0]\n                                            loc_loss += level_loss\n                                            loc_losses[image_type] += level_loss.item()\n                                \n                                elif image_type == 'Sagittal T1':\n                                    for side_idx, side in enumerate(['right', 'left']):\n                                        for level_idx, level in enumerate(LEVELS):\n                                            points = keypoints[side].get(level, [])\n                                            if len(points) > 0:\n                                                level_loss = localizer_criterion(\n                                                    sagittal_t1_keypoints[batch_idx, side_idx, level_idx],\n                                                    points) * mask[batch_idx, 1 if side == 'right' else 2]\n                            \n                                                loc_loss += level_loss\n                                                loc_losses[image_type] += level_loss.item()\n                                \n                                elif image_type == 'Axial T2':\n                                    for level_idx, level in enumerate(LEVELS):\n                                        sides = keypoints.get(level, {})\n                                        for side_idx, side in enumerate(['right', 'left']):\n                                            points = sides.get(side, [])\n                                            if len(points) > 0:\n                                                level_loss = localizer_criterion(\n                                                    axial_keypoints[batch_idx, level_idx, side_idx],\n                                                    points\n                                                ) * mask[batch_idx, 3 + level_idx]\n                                                loc_loss += level_loss\n                                                loc_losses[image_type] += level_loss.item()\n\n                    if not localizer_frozen:\n                        localizer_scaler.scale(loc_loss).backward() #(retain_graph=True)\n\n                    # Track localization losses\n                    total_loc_loss = sum(loc_losses.values())\n\n                    # Step 3: Crop images on GPU\n                    cropped_images, keypoint_masks = gpu_crop_region(images, keypoints_to_use, epoch=epoch)\n\n                    # Step 4: Classification\n                    with autocast:\n                        class_outputs = model(cropped_images, task='classification')\n                        class_loss, individual_losses = classifier_criterion(class_outputs, class_labels)\n                        \n                    for k, v in individual_losses.items():\n                        class_losses[k] += v\n                    \n                    classifier_scaler.scale(class_loss).backward()\n\n                    total_loc_loss += loc_loss.item() if not localizer_frozen else 0\n                    total_class_loss += class_loss.item()\n\n#                     if GRAD_ACCUMULATION > 1:\n#                         total_loc_loss = total_loc_loss / GRAD_ACCUMULATION\n#                         total_class_loss = total_class_loss / GRAD_ACCUMULATION\n\n                if not math.isfinite(class_loss.item()):\n                    print(f\"Loss is {class_loss.item()}, stopping training\")\n                    sys.exit(1)\n                \n                loaded_items.set_postfix(\n                    OrderedDict(\n                        class_loss=f'{class_loss.item():.6f}',\n                        loc_loss=f'{loc_loss.item():.6f}' if not localizer_frozen else 'N/A',\n                        lr=f'{classification_optimizer.param_groups[0][\"lr\"]:.3e}'\n                    )\n                )\n\n                if clip_and_count(keypoint_params, MAX_GRAD_NORM):\n                    keypoint_clip_count += 1\n                    \n                if clip_and_count(classification_params, MAX_GRAD_NORM):\n                    classification_clip_count += 1\n                \n                if (idx + 1) % GRAD_ACCUMULATION == 0:\n                    if not localizer_frozen:\n                        localizer_scaler.step(keypoint_optimizer)\n                        localizer_scaler.update()\n                    classifier_scaler.step(classification_optimizer)\n                    classifier_scaler.update()\n                    keypoint_optimizer.zero_grad()\n                    classification_optimizer.zero_grad()\n                \n        train_class_loss = total_class_loss / len(training_dataloader)\n        train_loc_loss = total_loc_loss / len(training_dataloader) if not localizer_frozen else 0\n        epoch_metrics['train_class_loss'].append(train_class_loss)\n        epoch_metrics['train_loc_loss'].append(train_loc_loss)\n        for k in loc_losses.keys():\n            epoch_metrics['train_loc_losses'][k].append(loc_losses[k] / len(training_dataloader))\n            epoch_metrics['train_class_losses'][k].append(class_losses[k] / len(training_dataloader))\n        \n        # Validation phase\n        model.eval()\n        total_val_class_loss = 0\n        total_val_loc_loss = 0\n        val_loc_losses = {'Sagittal T2': 0, 'Sagittal T1': 0, 'Axial T2': 0}\n        val_class_losses = {'Sagittal T2': 0, 'Sagittal T1': 0, 'Axial T2': 0}\n        all_predictions = []\n        all_labels = []\n        \n        with torch.no_grad():\n            for idx, batch in tqdm(enumerate(validation_dataloader), leave=False, desc=\"Validation\"):\n                images = batch['images'].to(device)\n                keypoint_labels = batch['keypoints']\n                class_labels = batch['labels'].to(device)\n                mask = batch['mask'].to(device)\n\n                with autocast:\n                    # Localization\n                    predicted_keypoints = model(images, task='localization')\n                    \n                    # Unpack the nested tensor\n                    sagittal_t2_keypoints, sagittal_t1_keypoints, axial_keypoints = predicted_keypoints.unbind()\n                    sagittal_t2_keypoints = sagittal_t2_keypoints.squeeze()\n                    \n                    # Compute localization loss using keypoint_masks\n                    loc_loss = 0\n                    for batch_idx, batch_item in enumerate(keypoint_labels):\n                        for image_type, keypoints in batch_item.items():\n                            if image_type == 'Sagittal T2':\n                                for level_idx, level in enumerate(LEVELS):\n                                    points = keypoints.get(level, [])\n                                    if len(points) > 0:\n                                        level_loss = localizer_criterion(\n                                            sagittal_t2_keypoints[batch_idx, level_idx],\n                                            points_to_tensor(points)\n                                        ) * mask[batch_idx, 0]\n                                        loc_loss += level_loss\n                                        val_loc_losses['Sagittal T2'] += level_loss.item()\n                            \n                            elif image_type == 'Sagittal T1':\n                                for side_idx, side in enumerate(['right', 'left']):\n                                    for level_idx, level in enumerate(LEVELS):\n                                        points = keypoints[side].get(level, [])\n                                        if len(points) > 0:\n                                            level_loss = localizer_criterion(\n                                                sagittal_t1_keypoints[batch_idx, side_idx, level_idx],\n                                                points_to_tensor(points)\n                                            ) * mask[batch_idx, 1 if side == 'right' else 2]\n                                            loc_loss += level_loss\n                                            val_loc_losses['Sagittal T1'] += level_loss.item()\n                            \n                            elif image_type == 'Axial T2':\n                                for level_idx, level in enumerate(LEVELS):\n                                    sides = keypoints.get(level, {})\n                                    for side_idx, side in enumerate(['right', 'left']):\n                                        points = sides.get(side, [])\n                                        if len(points) > 0:\n                                            level_loss = localizer_criterion(\n                                                axial_keypoints[batch_idx, level_idx, side_idx],\n                                                points_to_tensor(points)\n                                            ) * mask[batch_idx, 3 + level_idx]\n                                            loc_loss += level_loss\n                                            val_loc_losses['Axial T2'] += level_loss.item()\n                    \n                # Crop images\n                cropped_images, keypoint_masks = gpu_crop_region(images, keypoints_to_use, epoch=epoch)\n\n                with autocast:\n                    class_outputs = model(cropped_images, task='classification')\n                    class_loss, individual_losses = classifier_criterion(class_outputs, class_labels)\n\n                for k, v in individual_losses.items():\n                    val_class_losses[k] += v\n\n                total_val_loc_loss += loc_loss.item()\n                total_val_class_loss += class_loss.item()\n\n                all_predictions.append(class_outputs.cpu())\n                all_labels.append(class_labels.cpu())\n                \n                # plot for monitoring\n#                 if idx == 0:\n#                     if (epoch - 1) % 6 == 0:\n#                         save_crops_for_study(cropped_images, current_validation_index[idx], epoch, OUTPUT_DIR)\n                    \n#                     if epoch % 2 == 0:\n#                         visualize_keypoints(model, images, keypoint_labels[0], device, epoch, fold, OUTPUT_DIR, study_id_for_kp_plotting)\n\n            validation_class_loss = total_val_class_loss / len(validation_dataloader)\n            validation_loc_loss = total_val_loc_loss / len(validation_dataloader) if not localizer_frozen else 0\n        \n            # take steps with the schedulers\n            keypoint_scheduler.step(total_val_loc_loss)\n            classification_scheduler.step(total_val_class_loss)\n\n            all_predictions = torch.cat(all_predictions, dim=0)\n            all_labels = torch.cat(all_labels, dim=0)\n            \n            all_predictions = all_predictions.to(device).to(torch.float32)\n            all_labels = all_labels.to(device).to(torch.long)\n\n            # Calculate validation metrics\n            validation_wall = classifier_criterion(all_predictions, all_labels)[0].item()\n\n            epoch_metrics['val_class_loss'].append(validation_class_loss)\n            epoch_metrics['val_loc_loss'].append(validation_loc_loss)\n            for k in val_loc_losses.keys():\n                epoch_metrics['val_loc_losses'][k].append(val_loc_losses[k] / len(validation_dataloader))\n                epoch_metrics['val_class_losses'][k].append(val_class_losses[k] / len(validation_dataloader))\n\n            # Print summary\n            print(f'\\n{\"=\"*50}')\n            print(f'Epoch {epoch} Summary:')\n            print(f'{\"=\"*50}')\n\n            print(f'\\nTraining Losses:')\n            print(f'  Total Classification Loss: {train_class_loss:.6f}')\n            print(f'  Total Localization Loss: {train_loc_loss:.6f}')\n            print(f'  Total Classification clip count: {classification_clip_count}')\n            print(f'  Total Localization clip count: {keypoint_clip_count}')\n            print(f'  Total Localization Loss: {train_loc_loss:.6f}')\n            print(f'  Localization Losses:')\n            for k, v in loc_losses.items():\n                avg_loss = v / len(training_dataloader)\n                print(f'    {k}: {avg_loss:.6f}')\n            print(f'  Classification Losses:')\n            for k, v in class_losses.items():\n                avg_loss = v / len(training_dataloader)\n                print(f'    {k}: {avg_loss:.6f}')\n\n            print(f'\\nValidation Losses:')\n            print(f'  Total Classification Loss: {validation_class_loss:.6f}')\n            print(f'  Total Localization Loss: {validation_loc_loss:.6f}')\n            print(f'  Validation Wall: {validation_wall:.6f}')\n            print(f'  Localization Losses:')\n            for k, v in val_loc_losses.items():\n                avg_loss = v / len(validation_dataloader)\n                print(f'    {k}: {avg_loss:.6f}')\n            print(f'  Classification Losses:')\n            for k, v in val_class_losses.items():\n                avg_loss = v / len(validation_dataloader)\n                print(f'    {k}: {avg_loss:.6f}')\n\n            print(f'\\nLearning Rate: {classification_optimizer.param_groups[0][\"lr\"]:.3e}')\n\n            if full_dataset_used:\n                print('\\nFull dataset is being used.')\n            else:\n                print('\\nPartial dataset is being used.')\n\n            if localizer_frozen:\n                print('Localizer is frozen.')\n            else:\n                print('Localizer is trainable.')\n\n            print(f'{\"=\"*50}\\n')\n\n            # Check localizer performance and update dataset/freeze localizer if needed\n            if not full_dataset_used and validation_loc_loss < localizer_performance_threshold:\n                print(\"Localizer performance threshold reached. Switching to full dataset.\")\n                training_rows = train_df.iloc[training_index]\n                validation_rows = train_df.iloc[validation_index]\n                print('Training length: ', len(training_rows), 'Validation length: ', len(validation_rows))\n                training_dataset.extend_dataset(training_rows)\n                training_dataloader = DataLoader(\n                    training_dataset,\n                    batch_size=BATCH_SIZE,\n                    shuffle=True,\n                    pin_memory=True,\n                    drop_last=True,\n                    num_workers=N_WORKERS,\n                    collate_fn=custom_collate\n                )\n                validation_dataset.extend_dataset(validation_rows)\n                validation_dataloader = DataLoader(\n                    validation_dataset,\n                    batch_size=BATCH_SIZE,\n                    shuffle=False,\n                    pin_memory=True,\n                    drop_last=False,\n                    num_workers=N_WORKERS,\n                    collate_fn=custom_collate\n                )\n                full_dataset_used = True\n\n            if not localizer_frozen and validation_loc_loss < localizer_freeze_threshold:\n                print(\"Localizer freeze threshold reached. Freezing localizer.\")\n                for param in model.sagittal_t2_keypoint_fc.parameters():\n                    param.requires_grad = False\n\n                for param in model.sagittal_t1_keypoint_fc.parameters():\n                    param.requires_grad = False\n\n                for param in model.axial_keypoint_fc.parameters():\n                    param.requires_grad = False\n                    \n                localizer_frozen = True\n\n            if validation_class_loss < best_loss or validation_wall < best_wall:\n                es_step = 0\n                if device != 'cuda:0':\n                    model.to('cuda:0')\n\n                if validation_class_loss < best_loss:\n                    print(f'epoch:{epoch}, best loss updated from {best_loss:.6f} to {validation_class_loss:.6f}')\n                    best_loss = validation_class_loss\n                    model_path = f'{OUTPUT_DIR}/best_loss_model_fold-{fold}.pt'\n                    torch.save(model.state_dict(), model_path)\n\n                if validation_wall < best_wall:\n                    print(f'epoch:{epoch}, best wall_metric updated from {best_wall:.6f} to {validation_wall:.6f}')\n                    best_wall = validation_wall\n                    model_path = f'{OUTPUT_DIR}/best_wall_model_fold-{fold}.pt'\n                    torch.save(model.state_dict(), model_path)\n\n                if device != 'cuda:0':\n                    model.to(device)\n\n            else:\n                es_step += 1\n                if es_step >= EARLY_STOPPING_EPOCH:\n                    print('Early stopping')\n                    break\n\n# After training, plot the metrics\nplt.figure(figsize=(20, 15))\n\n# Helper function to move data to CPU\ndef to_cpu(data):\n    if isinstance(data, torch.Tensor):\n        return data.cpu().numpy()\n    elif isinstance(data, list):\n        return [to_cpu(item) for item in data]\n    elif isinstance(data, dict):\n        return {k: to_cpu(v) for k, v in data.items()}\n    else:\n        return data\n\n# Move all data to CPU\nepoch_metrics = to_cpu(epoch_metrics)\n\n# Classification Losses\nplt.subplot(2, 2, 1)\nplt.plot(epoch_metrics['train_class_loss'], label='Train Class Loss')\nplt.plot(epoch_metrics['val_class_loss'], label='Val Class Loss')\nplt.plot(epoch_metrics['val_wall'], label='Validation Wall')\nplt.title('Classification Losses')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\n# Localization Losses\nplt.subplot(2, 2, 2)\nplt.plot(epoch_metrics['train_loc_loss'], label='Train Loc Loss')\nplt.plot(epoch_metrics['val_loc_loss'], label='Val Loc Loss')\nplt.title('Localization Losses')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\n# Detailed Classification Losses\nplt.subplot(2, 2, 3)\nfor k in epoch_metrics['train_class_losses'].keys():\n    plt.plot(epoch_metrics['train_class_losses'][k], label=f'Train {k}')\n    plt.plot(epoch_metrics['val_class_losses'][k], label=f'Val {k}')\nplt.title('Detailed Classification Losses')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\n# Detailed Localization Losses\nplt.subplot(2, 2, 4)\nfor k in epoch_metrics['train_loc_losses'].keys():\n    plt.plot(epoch_metrics['train_loc_losses'][k], label=f'Train {k}')\n    plt.plot(epoch_metrics['val_loc_losses'][k], label=f'Val {k}')\nplt.title('Detailed Localization Losses')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.savefig(f'{OUTPUT_DIR}/metrics_fold-{fold}.png')\nplt.close()\n\n# Validation Classification Losses Heatmap\nplt.figure(figsize=(10, 8))\nval_class_losses_df = pd.DataFrame(epoch_metrics['val_class_losses'])\nsns.heatmap(val_class_losses_df.T, annot=True, cmap='YlOrRd')\nplt.title('Validation Classification Losses Heatmap')\nplt.tight_layout()\nplt.savefig(f'{OUTPUT_DIR}/val_class_losses_heatmap_fold-{fold}.png')\nplt.close()\n\nprint(\"Training completed. Metrics plots saved.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:26:57.538733Z","iopub.execute_input":"2024-10-02T09:26:57.539160Z","iopub.status.idle":"2024-10-02T09:28:49.689966Z","shell.execute_reply.started":"2024-10-02T09:26:57.539117Z","shell.execute_reply":"2024-10-02T09:28:49.688884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '/kaggle/working/rsna24-results/keypoint_visualization_study_8785691_epoch_2_fold-0.png'\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# Load the image\nimg = mpimg.imread(path)\n\n# Display the image\nplt.figure(figsize=(20, 15))  # Optional: set the figure size\nplt.imshow(img)\nplt.axis('off')  # Turn off axis numbers and ticks\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-02T09:24:09.931162Z","iopub.status.idle":"2024-10-02T09:24:09.931692Z","shell.execute_reply.started":"2024-10-02T09:24:09.931433Z","shell.execute_reply":"2024-10-02T09:24:09.931458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}